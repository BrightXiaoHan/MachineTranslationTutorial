
# Truecase

åœ¨è‹±è¯­ç­‰ä¸€äº›å¤§å°å†™æ•æ„Ÿçš„è¯­è¨€ä¸­ï¼Œä¸€äº›ä¸“æœ‰åè¯å’Œæœ‰ç‰¹æ®Šç”¨æ³•çš„å•è¯ï¼Œä»¥åŠæ¯ä¸ªå¥å­çš„é¦–å­—æ¯éƒ½éœ€è¦è¿›è¡Œå¤§å†™ã€‚æ­¤å¤–ï¼Œè®­ç»ƒæ•°æ®ä¸­ä¹Ÿä¼šåŒ…æ‹¬ä¸€äº›å¤§å°å†™é”™è¯¯çš„ç”¨æ³•ã€‚è¿™å¯¼è‡´è®¸å¤šå•è¯ç”±äºå¤§å°å†™çš„åŒºåˆ†å­˜åœ¨å¤šç§å½¢å¼ã€‚ä¸€ç§ç®€å•çš„åšæ³•æ˜¯å°†æ•°æ®å…¨éƒ¨è¿›è¡Œå°å†™åŒ–ï¼Œè¿™æ ·å¯ä»¥ä½¿æ‰€æœ‰çš„å•è¯è¿›è¡Œç»Ÿä¸€ï¼Œå¤§å¤§æå‡æ¨¡å‹é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”¨å°å†™åŒ–æ•°æ®è®­ç»ƒçš„æ¨¡å‹ç¿»è¯‘ç»“æœä¹Ÿéƒ½æ˜¯å°å†™çš„ï¼Œéœ€è¦é¢å¤–çš„è¿˜åŸæ¨¡å‹å¯¹ç»“æœè¿›è¡Œå¤„ç†ã€‚

 ç°åœ¨æ›´å¸¸ç”¨çš„åšæ³•æ˜¯ä¿ç•™å¥å­ä¸­æ¯ä¸ªå•è¯çš„æ­£ç¡®å¤§å°å†™å½¢å¼ã€‚ä½†æ˜¯å¯¹äºå¥å­çš„é¦–å­—æ¯ï¼Œéœ€å°†å…¶è½¬æ¢æˆè¿™ä¸ªå•è¯æœ€å¸¸è§çš„å½¢å¼ï¼Œå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚
 
 What is the WTO ? 
 
 - Lowercase: what is the wto ?
 - Truecase: what is the WTO ? 


é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè®­ç»ƒæ•°æ®ä¸­åªåŒ…å«å•è¯çš„æ­£ç¡®å¤§å°å†™å½¢å¼ï¼Œå¤§å†™å•è¯åªå­˜åœ¨äºä¸€äº›ä¸“æœ‰åè¯æˆ–è€…æœ‰ç‰¹æ®Šç”¨æ³•çš„å•è¯ä¸­ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°äº†è¯è¡¨å¤§å°ï¼ŒåŒæ—¶ï¼Œä¹Ÿå»é™¤äº†ä¸€éƒ¨åˆ†æ•°æ®ä¸­ç”±äºé”™è¯¯å¤§å°å†™å½¢å¼æ‰€äº§ç”Ÿçš„å™ªéŸ³ã€‚åœ¨ç¿»è¯‘ç»“æŸåï¼Œå¯¹é¦–å­—æ¯è¿›è¡Œå¤§å†™å°±èƒ½å¾—åˆ°å¤§å°å†™åˆç†çš„ç¿»è¯‘ç»“æœã€‚å¦å¤–ï¼Œä¸­æ–‡å­˜åœ¨ç®€ç¹ä½“ä¸¤ç§å½¢å¼çš„æ±‰å­—ï¼Œè®­ç»ƒæ•°æ®ä¸­å¯èƒ½ä¼šåŒæ—¶åŒ…å«è¿™ä¸¤ç§å½¢å¼ã€‚å› æ­¤é€šå¸¸ä¹Ÿä¼šè€ƒè™‘æŠŠç¹ä½“ä¸­æ–‡è½¬åŒ–ä¸ºç®€ä½“ä¸­æ–‡ï¼Œä»¥ç»Ÿä¸€æ±‰å­—çš„ç¼–ç ã€‚
 

æœ¬èŠ‚ä¸»è¦ä»‹ç»å¦‚ä½•è®­ç»ƒTruecaseæ¨¡å‹ï¼Œå¯¹è®­ç»ƒæ•°æ®è¿›è¡ŒTruecaseå¤„ç†ï¼Œä»¥åŠå¯¹Truecaseæ•°æ®è¿›è¡Œè¿˜åŸï¼ˆDetruecaseï¼‰ã€‚

## è®­ç»ƒTruecaseæ¨¡å‹
ç”±äºTruecaseæ˜¯é’ˆå¯¹æŸä¸€ç§è¯­è¨€çš„ï¼Œå¹¶ä¸è¦æ±‚ä¸€å®šè¦ä½¿ç”¨åŒè¯­è¯­æ–™è¿›è¡Œè®­ç»ƒï¼Œè¿˜å¯ä»¥åˆ©ç”¨è·å–æˆæœ¬è¾ƒä½çš„å•è¯­è¯­æ–™è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬é¦–å…ˆå‡†å¤‡ä¸€ä¸ªå°çš„æ•°æ®é›†æ¥åšå®éªŒã€‚ç”±äºTruecaseæ˜¯ä»¥è¯ä¸ºå•ä½è¿›è¡Œå­¦ä¹ è®­ç»ƒçš„ï¼Œæ‰€ä»¥åœ¨åšTruecaseä¹‹å‰ï¼Œå…ˆè¦å¯¹è¯­æ–™è¿›è¡Œåˆ†è¯å¤„ç†ã€‚è¿™é‡Œä½¿ç”¨sacremosesä¸­çš„åˆ†è¯è„šæœ¬è¿›è¡Œåˆ†è¯ã€‚å…·ä½“çš„åˆ†è¯æµç¨‹ä¸åŸç†ï¼Œåœ¨EnglishTokenizerç« èŠ‚ä¸­è¿›è¡Œäº†è¯¦ç»†çš„ä»‹ç»ã€‚


```python
# å®‰è£…sacremoses
!pip -q install -i https://pypi.douban.com/simple sacremoses 
# è·å–è®­ç»ƒæ•°æ®
!wget -q https://gist.githubusercontent.com/alvations/6e878bab0eda2624167aa7ec13fc3e94/raw/4fb3bac1da1ba7a172ff1936e96bee3bc8892931/big.txt
# å¯¹æ•°æ®è¿›è¡Œåˆ†è¯å¤„ç†
!sacremoses -l en -j 4 tokenize  < big.txt > big.txt.tok
```

    [33mWARNING: You are using pip version 20.2.4; however, version 20.3.1 is available.
    You should consider upgrading via the '/root/Softwares/anaconda3/bin/python -m pip install --upgrade pip' command.[0m
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128457/128457 [00:15<00:00, 8344.83it/s]



```python
!head big.txt.tok
```

    The Project Gutenberg EBook of The Adventures of Sherlock Holmes
    by Sir Arthur Conan Doyle
    ( # 15 in our series by Sir Arthur Conan Doyle )
    
    Copyright laws are changing all over the world . Be sure to check the
    copyright laws for your country before downloading or redistributing
    this or any other Project Gutenberg eBook .
    
    This header should be the first thing seen when viewing this Project
    Gutenberg file . Please do not remove it . Do not change or edit the


è®­ç»ƒTruecaseæ¨¡å‹çš„åŸç†å…¶å®éå¸¸ç®€å•ï¼Œæˆ‘ä»¬åªéœ€è¦ç»Ÿè®¡æ¯ä¸ªå•è¯ä¸åŒå½¢æ€ä¸‹çš„è¯é¢‘ã€‚æ¯”å¦‚å•è¯ â€œinternetâ€ï¼Œåœ¨æˆ‘ä»¬çš„è®­ç»ƒè¯­æ–™ä¸­æœ‰ä¸‰ç§å½¢æ€ï¼Œåˆ†åˆ«æ˜¯â€œinternetâ€ï¼Œâ€œInternetâ€ï¼Œâ€œINTERNETâ€ï¼Œè¿™ä¸‰ç§å½¢æ€åœ¨è®­ç»ƒè¯­æ–™ä¸­å‡ºç°çš„é¢‘ç‡åˆ†åˆ«æ˜¯1ï¼Œ100ï¼Œ2æ¬¡ã€‚å½“æ¨¡å‹ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°è¿™ç§åˆ†å¸ƒç‰¹å¾ä¹‹åï¼Œåœ¨å¹³è¡Œè¯­æ–™é¢„å¤„ç†ã€åå¤„ç†é˜¶æ®µï¼Œå°±èƒ½å¯¹ä¸åŒCaseçš„â€œinternetâ€è¿›è¡Œå¤„ç†ï¼ˆå…·ä½“å¤„ç†æ–¹æ³•ç»†èŠ‚åé¢ä¼šè®²ï¼‰ã€‚

é¦–å…ˆæˆ‘ä»¬ç¼–å†™ç»Ÿè®¡ä¸€å¥è¯ä¸­æ¯ä¸ªè¯ä¸åŒå½¢å¼çš„è¯é¢‘çš„ä»£ç ã€‚



```python
import re

# å¦‚æœé‡åˆ°è¿™äº›è¯ï¼Œè¿™äº›è¯ä¸èƒ½ä½œä¸ºå¥å­çš„å¼€å¤´ï¼Œé€šå¸¸ä¸‹ä¸€ä¸ªè¯æ‰æ˜¯ã€‚å¦‚â€œ( Additional editing by Jose Menendez )â€
DELAYED_SENT_START = {
    "(",
    "[",
    '"',
    "'",
    "&apos;",
    "&quot;",
    "&#91;",
    "&#93;",
}

# å¦‚æœé‡åˆ°è¿™äº›è¯æ„å‘³ç€å½“å‰å¥å­ç»“æŸï¼Œä¸‹ä¸€ä¸ªå•è¯å¯èƒ½æ˜¯å¥å­çš„å¼€å¤´ã€‚
SENT_END = {".", ":", "?", "!"}

# è¯¥æ­£åˆ™ç”¨äºè·³è¿‡ä¸åŒ…å«å¤§å†™å­—æ¯ã€å°å†™å­—æ¯å’Œæ ‡é¢˜å­—æ¯çš„è¯ã€‚å¦‚çº¯æ•°å­—ï¼Œçº¯ç¬¦å· â€œ( # 15 in our series by Sir Arthur Conan Doyle )â€
Lowercase_Letter = open("assets/Lowercase_Letter.txt").read()
Uppercase_Letter = open("assets/Uppercase_Letter.txt").read()
Titlecase_Letter = open("assets/Titlecase_Letter.txt").read()

SKIP_LETTERS_REGEX = re.compile(
    u"[{}{}{}]".format(
        Lowercase_Letter, Uppercase_Letter, Titlecase_Letter
    )
)

def learn_truecase_weights(tokens):
    """
    tokens: å¥å­çš„åˆ†è¯ç»“æœ.
    """
    # ä¸‹ä¸€ä¸ªè¯æ˜¯å¦æ˜¯å¥é¦–å•è¯çš„æ ‡è®°ï¼Œå¦‚æœæ˜¯å¥é¦–å•è¯å¯èƒ½ä¸è®¡å…¥ç»Ÿè®¡
    is_first_word = True
    truecase_weights = []
    for i, token in enumerate(tokens):
        # è·³è¿‡xmlæ ‡è®°ä¸­çš„è¯ã€‚è¿™äº›è¯åœ¨åˆ†è¯æ—¶å¾€å¾€æ˜¯ä¸€ä¸ªæ•´ä½“ï¼Œé‡Œé¢çš„è¯çš„Caseä¸å¥é¦–è¯è¯­Caseä¸€æ ·æ²¡æœ‰ç»Ÿè®¡æ„ä¹‰ã€‚
        if re.search(r"(<\S[^>]*>)", token):
            continue
        # å¦‚æœé‡åˆ°è¿™äº›è¯ï¼Œè¿™äº›è¯ä¸èƒ½ä½œä¸ºå¥å­çš„å¼€å¤´ï¼Œé€šå¸¸ä¸‹ä¸€ä¸ªè¯æ‰æ˜¯ã€‚å¦‚â€œ( Additional editing by Jose Menendez )â€
        elif token in DELAYED_SENT_START:
            continue

        # å¦‚æœé‡åˆ°è¿™äº›è¯æ„å‘³ç€å½“å‰å¥å­ç»“æŸï¼Œä¸‹ä¸€ä¸ªå•è¯å¯èƒ½æ˜¯å¥å­çš„å¼€å¤´ã€‚é‡ç½® is_first_word
        if not is_first_word and token in SENT_END:
            is_first_word = True
            continue
        # è·³è¿‡ä¸éœ€è¦è¿›è¡Œå¤§å°å†™ç»Ÿè®¡çš„è¯ï¼Œå¦‚æ•°å­—ã€ç¬¦å·æˆ–è€…ä»–ä»¬çš„ç»„åˆ
        if not SKIP_LETTERS_REGEX.search(token):
            is_first_word = False
            continue

        # å°†å½“å‰è¯çš„ç»Ÿè®¡ç»“æœåŠ å…¥åˆ°truecase_weightsä¸­ã€‚å¦‚ (lowercasetoken, LowerCaseToken, 1)
        current_word_weight = 0
        if not is_first_word:
            current_word_weight = 1

        is_first_word = False

        if current_word_weight > 0:
            truecase_weights.append((token.lower(), token, current_word_weight))
    return truecase_weights

example = "Copyright laws are changing all over the world . Be sure to check the copyright laws for your country before downloading or redistributing this or any other Project Gutenberg eBook ."
learn_truecase_weights(example.split())
```




    [('laws', 'laws', 1),
     ('are', 'are', 1),
     ('changing', 'changing', 1),
     ('all', 'all', 1),
     ('over', 'over', 1),
     ('the', 'the', 1),
     ('world', 'world', 1),
     ('sure', 'sure', 1),
     ('to', 'to', 1),
     ('check', 'check', 1),
     ('the', 'the', 1),
     ('copyright', 'copyright', 1),
     ('laws', 'laws', 1),
     ('for', 'for', 1),
     ('your', 'your', 1),
     ('country', 'country', 1),
     ('before', 'before', 1),
     ('downloading', 'downloading', 1),
     ('or', 'or', 1),
     ('redistributing', 'redistributing', 1),
     ('this', 'this', 1),
     ('or', 'or', 1),
     ('any', 'any', 1),
     ('other', 'other', 1),
     ('project', 'Project', 1),
     ('gutenberg', 'Gutenberg', 1),
     ('ebook', 'eBook', 1)]



æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒè¯­æ–™ä¸­çš„æ¯ä¸€å¥è¯çš„è¯é¢‘åšç»Ÿè®¡ï¼Œå¹¶å°†ç»Ÿè®¡ç»“æœåˆå¹¶ã€‚


```python
# é¦–å…ˆè¯»å–è®­ç»ƒæ•°æ®
with open("big.txt.tok", 'r') as f:
    corpus = f.readlines()

from collections import defaultdict, Counter
# æ•°æ®ç»“æ„ç”¨äºç»Ÿè®¡æ¯ä¸ªå•è¯ä¸åŒè¯é¢‘
casing = defaultdict(Counter)

token_weights = []
for line in corpus:
    token_weights.extend(learn_truecase_weights(line.split()))

for lowercase_token, surface_token, weight in token_weights:
    casing[lowercase_token][surface_token] += weight

# å°†ç»Ÿè®¡ç»“æœåˆ†æˆbestï¼Œknownä¸¤éƒ¨åˆ†ã€‚bestè¡¨ç¤ºç»Ÿè®¡é¢‘æ•°æœ€é«˜çš„å¤§å°å†™å½¢å¼ï¼Œknowè¡¨ç¤ºå…¶ä»–çš„å¤§å°å†™å½¢å¼
best = {}
# æ­¤å¤„ä¸ºäº†ä¿è¯knowä¸­çš„æ¯ä¸ªå…ƒç´ å¯ä»¥é€šè¿‡å­—å…¸çš„å½¢å¼è®¿é—®ï¼Œæ‰€ä»¥è¿™é‡Œç”¨ä¸€ä¸ªCounterï¼Œæ¯ä¸ªå…ƒç´ çš„å€¼é»˜è®¤ä¸º1
known = Counter()

for token_lower in casing:
    tokens = casing[token_lower].most_common()
    best[token_lower] = tokens[0][0]
    for token, count in tokens[1:]:
        known[token] += 1
model = {"best": best, "known": known, "casing": casing}    
```

åœ¨è¿›è¡ŒTruecaseæ“ä½œå‰ï¼Œè¾“å…¥çš„æ–‡æœ¬é€šå¸¸æ˜¯ç»è¿‡åˆ†è¯å¤„ç†åçš„æ–‡æœ¬ï¼Œé¦–å…ˆå°†ä»–ä»¬ä»¥ç©ºæ ¼ä¸ºåˆ†éš”ç¬¦åˆ†æˆå•è¯ï¼ˆå¦‚æœæ–‡æœ¬ä¸­æœ‰xmlæ ¼å¼çš„æ–‡æœ¬ï¼Œä¹Ÿå°†å…¶åŒ…è£¹çš„å•è¯åˆ†å‰²å¼€æ¥ã€‚ï¼‰


```python
def split_xml(line):
    """
    å°†æ–‡æœ¬ä»¥ç©ºæ ¼ä¸ºåˆ†éš”å­—ç¬¦åˆ†å¼€ï¼Œå¦‚æœæ–‡æœ¬ä¸­åŒ…å«xmlæ ¼å¼çš„æ–‡æœ¬ï¼Œä¹Ÿå°†ä»–ä»¬åˆ†å¼€ã€‚
    å¦‚ hello <heading>Reminder</heading> ä¼šå°†å®ƒåˆ†å‰²æˆ
    ['hello', '<heading>', 'Reminder', '</heading>']
    """
    line = line.strip()
    tokens = []
    while line:
        # Assumes that xml tag is always separated by space.
        has_xml = re.search(r"^\s*(<\S[^>]*>)(.*)$", line)
        # non-XML test.
        is_non_xml = re.search(r"^\s*([^\s<>]+)(.*)$", line)
        # '<' or '>' occurs in word, but it's not an XML tag
        xml_cognates = re.search(r"^\s*(\S+)(.*)$", line)
        if has_xml:
            potential_xml, line_next = has_xml.groups()
            # exception for factor that is an XML tag
            if (
                re.search(r"^\S", line)
                and len(tokens) > 0
                and re.search(r"\|$", tokens[-1])
            ):
                tokens[-1] += potential_xml
                # If it's a token with factors, join with the previous token.
                is_factor = re.search(r"^(\|+)(.*)$", line_next)
                if is_factor:
                    tokens[-1] += is_factor.group(1)
                    line_next = is_factor.group(2)
            else:
                tokens.append(
                    potential_xml + " "
                )  # Token hack, unique to sacremoses.
            line = line_next

        elif is_non_xml:
            tokens.append(is_non_xml.group(1))  # Token hack, unique to sacremoses.
            line = is_non_xml.group(2)
        elif xml_cognates:
            tokens.append(
                xml_cognates.group(1)
            )  # Token hack, unique to sacremoses.
            line = xml_cognates.group(2)
        else:
            raise Exception("ERROR: huh? {}".format(line))
        tokens[-1] = tokens[-1].strip()  # Token hack, unique to sacremoses.
    return tokens

text = "hello <heading>Reminder</heading>"
split_xml(text)
```




    ['hello', '<heading>', 'Reminder', '</heading>']



æœ‰äº†æ¨¡å‹å’Œè¾“å…¥æ–‡æœ¬ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨æ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡ŒTruecaseå¤„ç†ã€‚


```python
def truecase(text, model, return_str=False, use_known=False):
    """
    å¯¹ä¸€å¥è¯æˆ–è€…ä¸€æ®µæ–‡æœ¬è¿›è¡ŒTruecaseæ“ä½œ
    
    Args:
        text (str): è¾“å…¥æ–‡æœ¬ï¼ˆå·²ç»ç»è¿‡åˆ†è¯å¤„ç†ï¼‰
        model (dict): ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°çš„caseçš„ç»Ÿè®¡æ•°æ®
        return_str (bool, optional): ä»¥strçš„å½¢å¼è¿”å›è¿˜æ˜¯ä»¥List[str]çš„å½¢å¼è¿”å›. Defaults to True.
        use_known (bool, optional): å½“è¯¥å‚æ•°ä¸ºTrueæ—¶ï¼Œå½“æŸä¸ªè¯ä¸æ˜¯å¥é¦–å•è¯ï¼Œå¹¶ä¸”æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°è¿‡çš„å¤§å°å†™å½¢å¼ï¼Œåˆ™ä¿ç•™åŸå¤§å°å†™å½¢å¼ä¸å˜ã€‚
                                    å½“è¯¥å‚æ•°ä¸ºFalseæ—¶ï¼Œä¼˜å…ˆä½¿ç”¨è¯¥è¯æœ€å¸¸è§çš„å¤§å°å†™å½¢å¼
    """
    # è®°å½•å½“å‰å•è¯æ˜¯å¦åº”ä¸ºå¥é¦–å•è¯
    is_first_word = True
    truecased_tokens = []
    tokens = split_xml(text)

    for i, token in enumerate(tokens):
        # è¿™é‡Œä»¥ â€|â€œ ç¬¦å·å¼€å¤´çš„å•è¯ä¸åšå¤„ç†ã€‚æ³¨ï¼šè¿™é‡Œä¸ºä»€ä¹ˆè¦å¯¹è¿™ä¸ªç¬¦å·åšç‰¹æ®Šå¤„ç†è¿˜ä¸å¤ªæ¸…é™¤
        if token == "|" or token.startswith("|"):
            truecased_tokens.append(token)
            continue
        
        # å¤„ç†è¿™ç§æƒ…å†µ  â€hello|thankyouâ€œ -> token="hello", other_fectors="|thankyou"æ˜¯å¤„ç†è¯ä¸­æœ‰â€|ç¬¦å·çš„æƒ…å†µâ€œ
        token, other_factors = re.search(r"^([^\|]+)(.*)", token).groups()

        # æœ€å¸¸è§çš„ï¼ˆè®­ç»ƒä¸­é¢‘æ•°æœ€é«˜çš„ï¼‰å•è¯å¤§å°å†™å½¢å¼
        best_case = model["best"].get(token.lower(), None)
        # å…¶ä»–çš„å•è¯å¤§å°å†™å½¢å¼
        known_case = model["known"].get(token, None)
  
        if is_first_word and best_case:  # å¥é¦–å•è¯é‡‡ç”¨æœ€å¸¸è§çš„å¤§å°å†™å½¢å¼
            token = best_case
        elif known_case and use_known:  # åœ¨è®­ç»ƒé›†ä¸­å‡ºç°è¿‡çš„å¹¶ä¸”use_known=Trueå¤§å°å†™å½¢å¼ä¿æŒä¸å˜
            token = token
        elif (
            best_case
        ):  # å¦‚æœåŒ¹é…åˆ°best_caseä½¿ç”¨æœ€å¸¸è§çš„å¤§å°å†™å½¢å¼
            token = best_case
        # å¦åˆ™æ˜¯æ²¡æœ‰è§è¿‡çš„å•è¯ï¼Œå¤§å°å†™å½¢å¼ä¹Ÿä¿æŒä¸å˜
        
        # å¤„ç†ä¹‹å‰ä»¥â€|â€œå°†è¯åˆ†å¼€çš„æƒ…å†µï¼Œå°†ä»–ä»¬é‡æ–°æ‹¼æ¥åœ¨ä¸€èµ·
        token = token + other_factors
        # Adds the truecased
        truecased_tokens.append(token)

        # é‡è§å¥æœ«æ ‡ç‚¹é‡ç½®å¥é¦–æ ‡å¿—
        is_first_word = token in SENT_END
        
        # å»¶è¿Ÿå°†å¥é¦–æ ‡å¿—ç½®ä¸ºFalse
        if token in DELAYED_SENT_START:
            is_first_word = False

    # æ ¹æ®return_strå‚æ•°åˆ¤æ–­æ˜¯ä»¥è¯çš„å½¢å¼è¿”å›è¿˜æ˜¯ä»¥å­—ç¬¦ä¸²çš„å½¢å¼è¿”å›
    return " ".join(truecased_tokens) if return_str else truecased_tokens
```

æ‰¾ä¸€æ®µæ–‡æœ¬æ¥è¯•ä¸€ä¸‹æ•ˆæœ


```python
input_str = "You can also find out about how to make a donation to Project Gutenberg, and how to get involved."
output_str = truecase(input_str, model, return_str=True)
print(output_str)
```

    you can also find out about how to make a donation to Project Gutenberg, and how to get involved.


å¯ä»¥çœ‹åˆ°ï¼Œé¦–å­—æ¯Youå˜æˆäº†å°å†™ï¼ŒäººåProject Gutenbergè¿˜ä¿ç•™äº†åŸæ¥çš„å½¢å¼ã€‚
