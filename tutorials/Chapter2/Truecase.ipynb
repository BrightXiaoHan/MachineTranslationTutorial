{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truecase\n",
    "\n",
    "在英语等一些大小写敏感的语言中，一些专有名词和有特殊用法的单词，以及每个句子的首字母都需要进行大写。此外，训练数据中也会包括一些大小写错误的用法。这导致许多单词由于大小写的区分存在多种形式。一种简单的做法是将数据全部进行小写化，这样可以使所有的单词进行统一，大大提升模型预测的准确性。然而，用小写化数据训练的模型翻译结果也都是小写的，需要额外的还原模型对结果进行处理。\n",
    "\n",
    " 现在更常用的做法是保留句子中每个单词的正确大小写形式。但是对于句子的首字母，需将其转换成这个单词最常见的形式，如下表所示。\n",
    " \n",
    " What is the WTO ? \n",
    " \n",
    " - Lowercase: what is the wto ?\n",
    " - Truecase: what is the WTO ? \n",
    "\n",
    "\n",
    "通过这种方式，训练数据中只包含单词的正确大小写形式，大写单词只存在于一些专有名词或者有特殊用法的单词中，在一定程度上减小了词表大小，同时，也去除了一部分数据中由于错误大小写形式所产生的噪音。在翻译结束后，对首字母进行大写就能得到大小写合理的翻译结果。另外，中文存在简繁体两种形式的汉字，训练数据中可能会同时包含这两种形式。因此通常也会考虑把繁体中文转化为简体中文，以统一汉字的编码。\n",
    " \n",
    "\n",
    "本节主要介绍如何训练Truecase模型，对训练数据进行Truecase处理，以及对Truecase数据进行还原（Detruecase）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练Truecase模型\n",
    "由于Truecase是针对某一种语言的，并不要求一定要使用双语语料进行训练，还可以利用获取成本较低的单语语料进行训练。我们首先准备一个小的数据集来做实验。由于Truecase是以词为单位进行学习训练的，所以在做Truecase之前，先要对语料进行分词处理。这里使用sacremoses中的分词脚本进行分词。具体的分词流程与原理，在EnglishTokenizer章节中进行了详细的介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/root/Softwares/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "100%|█████████████████████████████████| 128457/128457 [00:15<00:00, 8344.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# 安装sacremoses\n",
    "!pip -q install -i https://pypi.douban.com/simple sacremoses \n",
    "# 获取训练数据\n",
    "!wget -q https://gist.githubusercontent.com/alvations/6e878bab0eda2624167aa7ec13fc3e94/raw/4fb3bac1da1ba7a172ff1936e96bee3bc8892931/big.txt\n",
    "# 对数据进行分词处理\n",
    "!sacremoses -l en -j 4 tokenize  < big.txt > big.txt.tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\r\n",
      "by Sir Arthur Conan Doyle\r\n",
      "( # 15 in our series by Sir Arthur Conan Doyle )\r\n",
      "\r\n",
      "Copyright laws are changing all over the world . Be sure to check the\r\n",
      "copyright laws for your country before downloading or redistributing\r\n",
      "this or any other Project Gutenberg eBook .\r\n",
      "\r\n",
      "This header should be the first thing seen when viewing this Project\r\n",
      "Gutenberg file . Please do not remove it . Do not change or edit the\r\n"
     ]
    }
   ],
   "source": [
    "!head big.txt.tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练Truecase模型的原理其实非常简单，我们只需要统计每个单词不同形态下的词频。比如单词 “internet”，在我们的训练语料中有三种形态，分别是“internet”，“Internet”，“INTERNET”，这三种形态在训练语料中出现的频率分别是1，100，2次。当模型从训练数据中学习到这种分布特征之后，在平行语料预处理、后处理阶段，就能对不同Case的“internet”进行处理（具体处理方法细节后面会讲）。\n",
    "\n",
    "首先我们编写统计一句话中每个词不同形式的词频的代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laws', 'laws', 1),\n",
       " ('are', 'are', 1),\n",
       " ('changing', 'changing', 1),\n",
       " ('all', 'all', 1),\n",
       " ('over', 'over', 1),\n",
       " ('the', 'the', 1),\n",
       " ('world', 'world', 1),\n",
       " ('sure', 'sure', 1),\n",
       " ('to', 'to', 1),\n",
       " ('check', 'check', 1),\n",
       " ('the', 'the', 1),\n",
       " ('copyright', 'copyright', 1),\n",
       " ('laws', 'laws', 1),\n",
       " ('for', 'for', 1),\n",
       " ('your', 'your', 1),\n",
       " ('country', 'country', 1),\n",
       " ('before', 'before', 1),\n",
       " ('downloading', 'downloading', 1),\n",
       " ('or', 'or', 1),\n",
       " ('redistributing', 'redistributing', 1),\n",
       " ('this', 'this', 1),\n",
       " ('or', 'or', 1),\n",
       " ('any', 'any', 1),\n",
       " ('other', 'other', 1),\n",
       " ('project', 'Project', 1),\n",
       " ('gutenberg', 'Gutenberg', 1),\n",
       " ('ebook', 'eBook', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 如果遇到这些词，这些词不能作为句子的开头，通常下一个词才是。如“( Additional editing by Jose Menendez )”\n",
    "DELAYED_SENT_START = {\n",
    "    \"(\",\n",
    "    \"[\",\n",
    "    '\"',\n",
    "    \"'\",\n",
    "    \"&apos;\",\n",
    "    \"&quot;\",\n",
    "    \"&#91;\",\n",
    "    \"&#93;\",\n",
    "}\n",
    "\n",
    "# 如果遇到这些词意味着当前句子结束，下一个单词可能是句子的开头。\n",
    "SENT_END = {\".\", \":\", \"?\", \"!\"}\n",
    "\n",
    "# 该正则用于跳过不包含大写字母、小写字母和标题字母的词。如纯数字，纯符号 “( # 15 in our series by Sir Arthur Conan Doyle )”\n",
    "Lowercase_Letter = open(\"assets/Lowercase_Letter.txt\").read()\n",
    "Uppercase_Letter = open(\"assets/Uppercase_Letter.txt\").read()\n",
    "Titlecase_Letter = open(\"assets/Titlecase_Letter.txt\").read()\n",
    "\n",
    "SKIP_LETTERS_REGEX = re.compile(\n",
    "    u\"[{}{}{}]\".format(\n",
    "        Lowercase_Letter, Uppercase_Letter, Titlecase_Letter\n",
    "    )\n",
    ")\n",
    "\n",
    "def learn_truecase_weights(tokens):\n",
    "    \"\"\"\n",
    "    tokens: 句子的分词结果.\n",
    "    \"\"\"\n",
    "    # 下一个词是否是句首单词的标记，如果是句首单词可能不计入统计\n",
    "    is_first_word = True\n",
    "    truecase_weights = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 跳过xml标记中的词。这些词在分词时往往是一个整体，里面的词的Case与句首词语Case一样没有统计意义。\n",
    "        if re.search(r\"(<\\S[^>]*>)\", token):\n",
    "            continue\n",
    "        # 如果遇到这些词，这些词不能作为句子的开头，通常下一个词才是。如“( Additional editing by Jose Menendez )”\n",
    "        elif token in DELAYED_SENT_START:\n",
    "            continue\n",
    "\n",
    "        # 如果遇到这些词意味着当前句子结束，下一个单词可能是句子的开头。重置 is_first_word\n",
    "        if not is_first_word and token in SENT_END:\n",
    "            is_first_word = True\n",
    "            continue\n",
    "        # 跳过不需要进行大小写统计的词，如数字、符号或者他们的组合\n",
    "        if not SKIP_LETTERS_REGEX.search(token):\n",
    "            is_first_word = False\n",
    "            continue\n",
    "\n",
    "        # 将当前词的统计结果加入到truecase_weights中。如 (lowercasetoken, LowerCaseToken, 1)\n",
    "        current_word_weight = 0\n",
    "        if not is_first_word:\n",
    "            current_word_weight = 1\n",
    "\n",
    "        is_first_word = False\n",
    "\n",
    "        if current_word_weight > 0:\n",
    "            truecase_weights.append((token.lower(), token, current_word_weight))\n",
    "    return truecase_weights\n",
    "\n",
    "example = \"Copyright laws are changing all over the world . Be sure to check the copyright laws for your country before downloading or redistributing this or any other Project Gutenberg eBook .\"\n",
    "learn_truecase_weights(example.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们对训练语料中的每一句话的词频做统计，并将统计结果合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先读取训练数据\n",
    "with open(\"big.txt.tok\", 'r') as f:\n",
    "    corpus = f.readlines()\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "# 数据结构用于统计每个单词不同词频\n",
    "casing = defaultdict(Counter)\n",
    "\n",
    "token_weights = []\n",
    "for line in corpus:\n",
    "    token_weights.extend(learn_truecase_weights(line.split()))\n",
    "\n",
    "for lowercase_token, surface_token, weight in token_weights:\n",
    "    casing[lowercase_token][surface_token] += weight\n",
    "\n",
    "# 将统计结果分成best，known两部分。best表示统计频数最高的大小写形式，know表示其他的大小写形式\n",
    "best = {}\n",
    "# 此处为了保证know中的每个元素可以通过字典的形式访问，所以这里用一个Counter，每个元素的值默认为1\n",
    "known = Counter()\n",
    "\n",
    "for token_lower in casing:\n",
    "    tokens = casing[token_lower].most_common()\n",
    "    best[token_lower] = tokens[0][0]\n",
    "    for token, count in tokens[1:]:\n",
    "        known[token] += 1\n",
    "model = {\"best\": best, \"known\": known, \"casing\": casing}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行Truecase操作前，输入的文本通常是经过分词处理后的文本，首先将他们以空格为分隔符分成单词（如果文本中有xml格式的文本，也将其包裹的单词分割开来。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', '<heading>', 'Reminder', '</heading>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_xml(line):\n",
    "    \"\"\"\n",
    "    将文本以空格为分隔字符分开，如果文本中包含xml格式的文本，也将他们分开。\n",
    "    如 hello <heading>Reminder</heading> 会将它分割成\n",
    "    ['hello', '<heading>', 'Reminder', '</heading>']\n",
    "    \"\"\"\n",
    "    line = line.strip()\n",
    "    tokens = []\n",
    "    while line:\n",
    "        # Assumes that xml tag is always separated by space.\n",
    "        has_xml = re.search(r\"^\\s*(<\\S[^>]*>)(.*)$\", line)\n",
    "        # non-XML test.\n",
    "        is_non_xml = re.search(r\"^\\s*([^\\s<>]+)(.*)$\", line)\n",
    "        # '<' or '>' occurs in word, but it's not an XML tag\n",
    "        xml_cognates = re.search(r\"^\\s*(\\S+)(.*)$\", line)\n",
    "        if has_xml:\n",
    "            potential_xml, line_next = has_xml.groups()\n",
    "            # exception for factor that is an XML tag\n",
    "            if (\n",
    "                re.search(r\"^\\S\", line)\n",
    "                and len(tokens) > 0\n",
    "                and re.search(r\"\\|$\", tokens[-1])\n",
    "            ):\n",
    "                tokens[-1] += potential_xml\n",
    "                # If it's a token with factors, join with the previous token.\n",
    "                is_factor = re.search(r\"^(\\|+)(.*)$\", line_next)\n",
    "                if is_factor:\n",
    "                    tokens[-1] += is_factor.group(1)\n",
    "                    line_next = is_factor.group(2)\n",
    "            else:\n",
    "                tokens.append(\n",
    "                    potential_xml + \" \"\n",
    "                )  # Token hack, unique to sacremoses.\n",
    "            line = line_next\n",
    "\n",
    "        elif is_non_xml:\n",
    "            tokens.append(is_non_xml.group(1))  # Token hack, unique to sacremoses.\n",
    "            line = is_non_xml.group(2)\n",
    "        elif xml_cognates:\n",
    "            tokens.append(\n",
    "                xml_cognates.group(1)\n",
    "            )  # Token hack, unique to sacremoses.\n",
    "            line = xml_cognates.group(2)\n",
    "        else:\n",
    "            raise Exception(\"ERROR: huh? {}\".format(line))\n",
    "        tokens[-1] = tokens[-1].strip()  # Token hack, unique to sacremoses.\n",
    "    return tokens\n",
    "\n",
    "text = \"hello <heading>Reminder</heading>\"\n",
    "split_xml(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了模型和输入文本之后，我们就可以使用模型对文本进行Truecase处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truecase(text, model, return_str=False, use_known=False):\n",
    "    \"\"\"\n",
    "    对一句话或者一段文本进行Truecase操作\n",
    "    \n",
    "    Args:\n",
    "        text (str): 输入文本（已经经过分词处理）\n",
    "        model (dict): 从训练数据中学习到的case的统计数据\n",
    "        return_str (bool, optional): 以str的形式返回还是以List[str]的形式返回. Defaults to True.\n",
    "        use_known (bool, optional): 当该参数为True时，当某个词不是句首单词，并且是在训练数据中出现过的大小写形式，则保留原大小写形式不变。\n",
    "                                    当该参数为False时，优先使用该词最常见的大小写形式\n",
    "    \"\"\"\n",
    "    # 记录当前单词是否应为句首单词\n",
    "    is_first_word = True\n",
    "    truecased_tokens = []\n",
    "    tokens = split_xml(text)\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 这里以 ”|“ 符号开头的单词不做处理。注：这里为什么要对这个符号做特殊处理还不太清除\n",
    "        if token == \"|\" or token.startswith(\"|\"):\n",
    "            truecased_tokens.append(token)\n",
    "            continue\n",
    "        \n",
    "        # 处理这种情况  ”hello|thankyou“ -> token=\"hello\", other_fectors=\"|thankyou\"是处理词中有”|符号的情况“\n",
    "        token, other_factors = re.search(r\"^([^\\|]+)(.*)\", token).groups()\n",
    "\n",
    "        # 最常见的（训练中频数最高的）单词大小写形式\n",
    "        best_case = model[\"best\"].get(token.lower(), None)\n",
    "        # 其他的单词大小写形式\n",
    "        known_case = model[\"known\"].get(token, None)\n",
    "  \n",
    "        if is_first_word and best_case:  # 句首单词采用最常见的大小写形式\n",
    "            token = best_case\n",
    "        elif known_case and use_known:  # 在训练集中出现过的并且use_known=True大小写形式保持不变\n",
    "            token = token\n",
    "        elif (\n",
    "            best_case\n",
    "        ):  # 如果匹配到best_case使用最常见的大小写形式\n",
    "            token = best_case\n",
    "        # 否则是没有见过的单词，大小写形式也保持不变\n",
    "        \n",
    "        # 处理之前以”|“将词分开的情况，将他们重新拼接在一起\n",
    "        token = token + other_factors\n",
    "        # Adds the truecased\n",
    "        truecased_tokens.append(token)\n",
    "\n",
    "        # 遇见句末标点重置句首标志\n",
    "        is_first_word = token in SENT_END\n",
    "        \n",
    "        # 延迟将句首标志置为False\n",
    "        if token in DELAYED_SENT_START:\n",
    "            is_first_word = False\n",
    "\n",
    "    # 根据return_str参数判断是以词的形式返回还是以字符串的形式返回\n",
    "    return \" \".join(truecased_tokens) if return_str else truecased_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找一段文本来试一下效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you can also find out about how to make a donation to Project Gutenberg, and how to get involved.\n"
     ]
    }
   ],
   "source": [
    "input_str = \"You can also find out about how to make a donation to Project Gutenberg, and how to get involved.\"\n",
    "output_str = truecase(input_str, model, return_str=True)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，首字母You变成了小写，人名Project Gutenberg还保留了原来的形式。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
